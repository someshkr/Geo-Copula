{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/someshkr/Geo-Copula/blob/main/50kSimulation_space_time_DeepKriging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feeebfaa",
      "metadata": {
        "id": "feeebfaa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.python.keras.models import Sequential,Model\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, BatchNormalization,Input, LSTM, Add, Subtract, Lambda\n",
        "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.python.keras import regularizers,initializers\n",
        "import keras.backend as Kr\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "# Library for Gaussian process\n",
        "# import GPy\n",
        "##Library for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "import matplotlib;matplotlib.rcParams['figure.figsize'] = (10,10)\n",
        "import pylab\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# sess = tf.compat.v1.Session()\n",
        "# tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874e1191",
      "metadata": {
        "id": "874e1191"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.python.keras import backend as K\n",
        "\n",
        "# # adjust values to your needs\n",
        "# config = tf.compat.v1.ConfigProto( device_count = {'GPU': 2 , 'CPU': 30} )\n",
        "# sess = tf.compat.v1.Session(config=config)\n",
        "# K.set_session(sess)\n",
        "\n",
        "HeUniform = tf.keras.initializers.he_uniform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc5e252",
      "metadata": {
        "id": "ecc5e252"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"synthetic_50000.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606ee78c",
      "metadata": {
        "id": "606ee78c"
      },
      "outputs": [],
      "source": [
        "df[\"x\"] = df[\"V1\"]\n",
        "df[\"y\"] = df[\"V2\"]\n",
        "df[\"t\"] = df[\"V3\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b74cb64",
      "metadata": {
        "id": "6b74cb64"
      },
      "outputs": [],
      "source": [
        "# test locations\n",
        "a = np.linspace(0,1,100)\n",
        "\n",
        "s1, s2 = np.meshgrid(a,a)\n",
        "s_test = np.column_stack((s1.flatten(),s2.flatten()))\n",
        "s_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5aac03c",
      "metadata": {
        "id": "d5aac03c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "s = np.array(df[\"t\"]/500).reshape(len(df),1)\n",
        "print(s.shape)\n",
        "N_data = len(df)\n",
        "\n",
        "t = np.array([250,350,450])/500\n",
        "s_t = np.repeat(t, 10000).reshape(3*10000,1)\n",
        "print(s_t.shape)\n",
        "\n",
        "s = np.vstack((s,s_t))\n",
        "N = N_data + len(s_t)\n",
        "print(N)\n",
        "\n",
        "## time basis\n",
        "num_basis = [70,250,410]\n",
        "std_arr = [0.2,0.09,0.009]\n",
        "#std_arr = [0.3,0.15,0.05]\n",
        "mu_knots = [np.linspace(0,1,int(i)) for i in num_basis]\n",
        "\n",
        "phi_t = np.zeros((N, sum(num_basis)))\n",
        "K = 0\n",
        "for res in range(len(num_basis)):\n",
        "    std = std_arr[res]\n",
        "    for i in range(num_basis[res]):\n",
        "        d = np.square(np.absolute(s-mu_knots[res][i]))\n",
        "        for j in range(len(d)):\n",
        "            if d[j] >= 0 and d[j] <= 1:\n",
        "                phi_t[j,i + K] = np.exp(-0.5 * d[j]/(std**2))\n",
        "            else:\n",
        "                phi_t[j,i + K] = 0\n",
        "    K = K + num_basis[res]\n",
        "\n",
        "\n",
        "# # time basis\n",
        "# num_basis = [3,7,11]\n",
        "# knots = [np.linspace(0,1,i) for i in num_basis]\n",
        "# ##Wendland kernel\n",
        "# K = 0 ## basis size\n",
        "# phi_t = np.zeros((N, sum(num_basis)))\n",
        "# for res in range(len(num_basis)):\n",
        "#     theta = 1/num_basis[res]*2.5\n",
        "#     for i in range(num_basis[res]):\n",
        "#         d = np.absolute(s-knots[res][i])/theta\n",
        "#         for j in range(len(d)):\n",
        "#             if d[j] >= 0 and d[j] <= 1:\n",
        "#                 phi_t[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
        "#             else:\n",
        "#                 phi_t[j,i + K] = 0\n",
        "#     K = K + num_basis[res]\n",
        "\n",
        "s = np.vstack((df[\"x\"],df[\"y\"])).T\n",
        "\n",
        "s = np.vstack((s,s_test,s_test,s_test))\n",
        "\n",
        "# space basis\n",
        "num_basis = [5**2,9**2,11**2]\n",
        "knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
        "##Wendland kernel\n",
        "K = 0\n",
        "phi = np.zeros((N, sum(num_basis)))\n",
        "for res in range(len(num_basis)):\n",
        "    theta = 1/np.sqrt(num_basis[res])*2.5\n",
        "    knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
        "    knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
        "    for i in range(num_basis[res]):\n",
        "        d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
        "        for j in range(len(d)):\n",
        "            if d[j] >= 0 and d[j] <= 1:\n",
        "                phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
        "            else:\n",
        "                phi[j,i + K] = 0\n",
        "    K = K + num_basis[res]\n",
        "\n",
        "# phi2 = np.zeros((phi_t.shape[0],(phi_t.shape[1]*phi.shape[1])))\n",
        "\n",
        "# for i in range(phi_t.shape[0]):\n",
        "#     full_prod = []\n",
        "#     for j in range(len(phi_t[i])):\n",
        "#         if j == 0 : full_prod = phi[i]*phi_t[i,j]\n",
        "#         else:\n",
        "#             product = phi[i]*phi_t[i,j]\n",
        "#             full_prod = np.concatenate((full_prod, product), axis=None)\n",
        "#     phi2[i] = full_prod\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c311ec3d",
      "metadata": {
        "id": "c311ec3d"
      },
      "outputs": [],
      "source": [
        "print(phi_t.shape)\n",
        "print(phi.shape)\n",
        "\n",
        "phi2 = np.hstack((phi_t,phi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af24f71",
      "metadata": {
        "id": "4af24f71"
      },
      "outputs": [],
      "source": [
        "## Romove the all-zero columns\n",
        "idx_zero = np.array([], dtype=int)\n",
        "for i in range(phi2.shape[1]):\n",
        "    if sum(phi2[:,i]!=0)==0:\n",
        "        idx_zero = np.append(idx_zero,int(i))\n",
        "\n",
        "phi_reduce = np.delete(phi2,idx_zero,1)\n",
        "print(phi2.shape)\n",
        "print(phi_reduce.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff96466",
      "metadata": {
        "id": "bff96466"
      },
      "outputs": [],
      "source": [
        "np.save(\"embedding_50k.npy\", phi2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607926cf",
      "metadata": {
        "id": "607926cf"
      },
      "outputs": [],
      "source": [
        "y = np.array(df[\"nonstat_z\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420fa324",
      "metadata": {
        "id": "420fa324"
      },
      "outputs": [],
      "source": [
        "phi_reduce = np.load(\"embedding_50k.npy\")\n",
        "print(phi_reduce.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ab6c08",
      "metadata": {
        "id": "92ab6c08"
      },
      "outputs": [],
      "source": [
        "N_data = len(df)\n",
        "phi_reduce_train = phi_reduce[0:N_data,]\n",
        "phi_reduce_test = phi_reduce[N_data:phi_reduce.shape[0],]\n",
        "print(phi_reduce_test.shape)\n",
        "print(phi_reduce_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e47fdcb",
      "metadata": {
        "id": "8e47fdcb"
      },
      "source": [
        "## Deep Learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0818ba73",
      "metadata": {
        "id": "0818ba73"
      },
      "outputs": [],
      "source": [
        "## Functions for calculation of MSE and MAE\n",
        "def mse(y_pred,y_true):\n",
        "    mse = np.mean((y_pred-y_true)**2)\n",
        "    return mse\n",
        "\n",
        "def mae(y_pred,y_true):\n",
        "    mae = np.mean(np.absolute(y_pred-y_true))\n",
        "    return mae\n",
        "\n",
        "mse_var = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4d5ae6",
      "metadata": {
        "scrolled": true,
        "id": "fa4d5ae6"
      },
      "outputs": [],
      "source": [
        "#### Modeling the median\n",
        "start_time = time.time()\n",
        "x_train,x_test,y_train, y_test= train_test_split(phi_reduce_train, y,\n",
        "                                                        test_size=0.1)\n",
        "\n",
        "q = 0.5\n",
        "def tilted_loss(y,f):\n",
        "\n",
        "    e1 = (y-f)\n",
        "    the_sum = (Kr.mean(Kr.maximum(q*e1, (q-1)*e1), axis=-1))\n",
        "    return the_sum\n",
        "#     data_train = np.hstack((encoder_train,y_train))\n",
        "#     n_rows = data_train.shape[0]\n",
        "#     random_indices = np.random.choice(n_rows, size=10000, replace=True)\n",
        "#     resampled_data_train = data_train[random_indices, :]\n",
        "# DeepKriging model for continuous data\n",
        "model = Sequential()\n",
        "# model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
        "model.add(Dense(100, input_dim = x_train.shape[1],\n",
        "                kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(Dropout(rate=0.5))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4),\n",
        "                bias_regularizer=regularizers.l2(1e-4),\n",
        "                activity_regularizer=regularizers.l2(1e-5),activation='relu'))\n",
        "model.add(Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4),\n",
        "                bias_regularizer=regularizers.l2(1e-4),\n",
        "                activity_regularizer=regularizers.l2(1e-5),activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "# model.add(Dense(50, activation='relu'))\n",
        "#model.add(Dropout(rate=0.5))\n",
        "#     model.add(Dense(10, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation='linear'))\n",
        "NB_START_EPOCHS = 50\n",
        "# NB_START_EPOCHS = 200  # Number of epochs we usually start to train with\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss=tilted_loss)\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience= 30),\n",
        "         ModelCheckpoint(filepath='model_real-50k.h5', monitor='val_loss', save_best_only=True)]\n",
        "result = model.fit(x_train, y_train,callbacks = callbacks,\n",
        "                   validation_data=(x_test,y_test), epochs = 350, batch_size = 512, verbose = 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     result = model.fit(x_train, y_train, callbacks=callbacks,\n",
        "#                        validation_data=(x_test,y_test), epochs = 200, batch_size = 64, verbose = 2)\n",
        "model = keras.models.load_model('model_real-50k.h5',custom_objects={'tilted_loss':tilted_loss})\n",
        "# y_pred = model.predict(x_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "# mse_var.append(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"%s seconds\", end_time - start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d81b3d8a",
      "metadata": {
        "scrolled": true,
        "id": "d81b3d8a"
      },
      "outputs": [],
      "source": [
        "#### Modeling the quantile functions\n",
        "start_time = time.time()\n",
        "model1 = keras.models.load_model('model_real-50k.h5')\n",
        "medY = model1.predict(phi_reduce_train)\n",
        "x_train,x_test,y_train, y_test, medY_train, medY_test= train_test_split(phi_reduce_train, y, medY,\n",
        "                                                        test_size=0.1)\n",
        "\n",
        "\n",
        "\n",
        "q = 0.95\n",
        "def tilted_loss(y,f):\n",
        "\n",
        "    e1 = (y-f)\n",
        "    the_sum = (Kr.mean(Kr.maximum(q*e1, (q-1)*e1), axis=-1))\n",
        "    return the_sum\n",
        "#     data_train = np.hstack((encoder_train,y_train))\n",
        "#     n_rows = data_train.shape[0]\n",
        "#     random_indices = np.random.choice(n_rows, size=10000, replace=True)\n",
        "#     resampled_data_train = data_train[random_indices, :]\n",
        "# DeepKriging model for continuous data\n",
        "imp = Input(shape=x_train.shape[1])\n",
        "input_medY = Input(shape = 1)\n",
        "# model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
        "x = Dense(100,kernel_initializer='he_uniform', activation='relu')(imp)\n",
        "# model.add(Dropout(rate=0.5))\n",
        "# model.add(BatchNormalization())\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4),\n",
        "                bias_regularizer=regularizers.l2(1e-4),\n",
        "                activity_regularizer=regularizers.l2(1e-5),activation='relu')(x)\n",
        "x = Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4),\n",
        "                bias_regularizer=regularizers.l2(1e-4),\n",
        "                activity_regularizer=regularizers.l2(1e-5),activation='relu')(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "x = Dense(50, activation='relu')(x)\n",
        "x = Dense(50, activation='relu')(x)\n",
        "x = Dense(50, activation='relu')(x)\n",
        "x = Dense(50, activation='relu')(x)\n",
        "# model.add(Dense(50, activation='relu'))\n",
        "#model.add(Dropout(rate=0.5))\n",
        "#     model.add(Dense(10, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "x = Lambda(lambda d: d * 10)(x)\n",
        "x = Add()([input_medY,x]) #### comment out for quantile 95\n",
        "# x = Subtract()([input_medY,x]) #### comment out for quantile 05\n",
        "model = Model(inputs=[imp, input_medY], outputs=x)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss=tilted_loss)\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience= 30),\n",
        "         ModelCheckpoint(filepath='model_real-50k_95.h5', monitor='val_loss', save_best_only=True)]\n",
        "result = model.fit([x_train,medY_train], y_train,callbacks = callbacks,\n",
        "                   validation_data=([x_test,medY_test],y_test), epochs = 350, batch_size = 512, verbose = 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     result = model.fit(x_train, y_train, callbacks=callbacks,\n",
        "#                        validation_data=(x_test,y_test), epochs = 200, batch_size = 64, verbose = 2)\n",
        "# model = keras.models.load_model('model_real-50k_05.h5',custom_objects={'tilted_loss':tilted_loss})\n",
        "# y_pred = model.predict(x_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "# mse_var.append(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"%s seconds\", end_time - start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da607fc4",
      "metadata": {
        "id": "da607fc4"
      },
      "outputs": [],
      "source": [
        "q = 0.95\n",
        "def tilted_loss(y,f):\n",
        "\n",
        "    e1 = (y-f)\n",
        "    the_sum = (Kr.mean(Kr.maximum(q*e1, (q-1)*e1), axis=-1))\n",
        "    return the_sum\n",
        "model1 = keras.models.load_model('model_real-50k_05.h5',custom_objects={'tilted_loss':tilted_loss})\n",
        "model2 = keras.models.load_model('model_real-50k_95.h5',custom_objects={'tilted_loss':tilted_loss})\n",
        "model_med = keras.models.load_model('model_real-50k.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45519823",
      "metadata": {
        "id": "45519823"
      },
      "outputs": [],
      "source": [
        "## Locations for interpolation\n",
        "\n",
        "\n",
        "\n",
        "# t = (np.array(range(286)) + 1)/286\n",
        "s = np.array(np.unique(df[\"t\"]))/500\n",
        "\n",
        "# print(s_t.shape)\n",
        "\n",
        "# s = np.vstack((s,s_t))\n",
        "N = 500\n",
        "# print(N)\n",
        "\n",
        "## time basis\n",
        "num_basis = [70,250,410]\n",
        "std_arr = [0.2,0.09,0.009]\n",
        "#std_arr = [0.3,0.15,0.05]\n",
        "mu_knots = [np.linspace(0,1,int(i)) for i in num_basis]\n",
        "\n",
        "phi_t = np.zeros((N, sum(num_basis)))\n",
        "K = 0\n",
        "for res in range(len(num_basis)):\n",
        "    std = std_arr[res]\n",
        "    for i in range(num_basis[res]):\n",
        "        d = np.square(np.absolute(s-mu_knots[res][i]))\n",
        "        for j in range(len(d)):\n",
        "            if d[j] >= 0 and d[j] <= 1:\n",
        "                phi_t[j,i + K] = np.exp(-0.5 * d[j]/(std**2))\n",
        "            else:\n",
        "                phi_t[j,i + K] = 0\n",
        "    K = K + num_basis[res]\n",
        "\n",
        "\n",
        "# # time basis\n",
        "# num_basis = [3,7,11]\n",
        "# knots = [np.linspace(0,1,i) for i in num_basis]\n",
        "# ##Wendland kernel\n",
        "# K = 0 ## basis size\n",
        "# phi_t = np.zeros((N, sum(num_basis)))\n",
        "# for res in range(len(num_basis)):\n",
        "#     theta = 1/num_basis[res]*2.5\n",
        "#     for i in range(num_basis[res]):\n",
        "#         d = np.absolute(s-knots[res][i])/theta\n",
        "#         for j in range(len(d)):\n",
        "#             if d[j] >= 0 and d[j] <= 1:\n",
        "#                 phi_t[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
        "#             else:\n",
        "#                 phi_t[j,i + K] = 0\n",
        "#     K = K + num_basis[res]\n",
        "\n",
        "s = np.repeat([[0.077215,0.041551]],500).reshape(500,2)\n",
        "\n",
        "# space basis\n",
        "num_basis = [5**2,9**2,11**2]\n",
        "knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
        "##Wendland kernel\n",
        "K = 0\n",
        "phi = np.zeros((N, sum(num_basis)))\n",
        "for res in range(len(num_basis)):\n",
        "    theta = 1/np.sqrt(num_basis[res])*2.5\n",
        "    knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
        "    knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
        "    for i in range(num_basis[res]):\n",
        "        d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
        "        for j in range(len(d)):\n",
        "            if d[j] >= 0 and d[j] <= 1:\n",
        "                phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
        "            else:\n",
        "                phi[j,i + K] = 0\n",
        "    K = K + num_basis[res]\n",
        "\n",
        "\n",
        "phi_test = np.hstack((phi_t,phi))\n",
        "phi_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c971a839",
      "metadata": {
        "id": "c971a839"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(phi_reduce_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef76a81",
      "metadata": {
        "id": "0ef76a81"
      },
      "outputs": [],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d765e9",
      "metadata": {
        "id": "78d765e9"
      },
      "outputs": [],
      "source": [
        "#### For single location prediction\n",
        "\n",
        "df_pred1LOC = pd.DataFrame({\"pred\":predictions[:,0]})\n",
        "df_pred1LOC.to_csv(\"1Loc_interpolation.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f1dd274",
      "metadata": {
        "id": "5f1dd274"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame({\"x\":s_test[:,0],\"y\":s_test[:,1]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5e622c",
      "metadata": {
        "id": "ab5e622c"
      },
      "outputs": [],
      "source": [
        "df_pred[\"predictions_t250\"] = predictions[0:10000,]\n",
        "df_pred[\"predictions_t350\"] = predictions[10000:20000,]\n",
        "df_pred[\"predictions_t450\"] = predictions[20000:30000,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da61b11",
      "metadata": {
        "id": "3da61b11"
      },
      "outputs": [],
      "source": [
        "pred_med.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa97ae66",
      "metadata": {
        "id": "aa97ae66"
      },
      "outputs": [],
      "source": [
        "##### This section is only for storing the prediction intervals\n",
        "df_pred = pd.read_csv(\"50k_interpolation.csv\")\n",
        "pred_med = model_med.predict(phi_reduce_test)\n",
        "predictions1 = model1.predict([phi_reduce_test,pred_med])\n",
        "predictions2 = model2.predict([phi_reduce_test,pred_med])\n",
        "\n",
        "df_pred[\"lb_t250\"] = predictions1[0:10000,]\n",
        "df_pred[\"lb_t350\"] = predictions1[10000:20000,]\n",
        "df_pred[\"lb_t450\"] = predictions1[20000:30000,]\n",
        "\n",
        "df_pred[\"ub_t250\"] = predictions2[0:10000,]\n",
        "df_pred[\"ub_t350\"] = predictions2[10000:20000,]\n",
        "df_pred[\"ub_t450\"] = predictions2[20000:30000,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98bed11e",
      "metadata": {
        "id": "98bed11e"
      },
      "outputs": [],
      "source": [
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e274f502",
      "metadata": {
        "id": "e274f502"
      },
      "outputs": [],
      "source": [
        "np.mean(df_pred[\"ub_t250\"] - df_pred[\"lb_t250\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1107470b",
      "metadata": {
        "id": "1107470b"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv(\"50k_interpolation.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d025960",
      "metadata": {
        "id": "7d025960"
      },
      "source": [
        "## Forecasting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "635c6f91",
      "metadata": {
        "id": "635c6f91"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"50k_lstm_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcdbcd16",
      "metadata": {
        "id": "dcdbcd16"
      },
      "outputs": [],
      "source": [
        "Z = np.array(df.iloc[:,5])\n",
        "\n",
        "split_size = 495\n",
        "train = Z[:split_size].reshape(split_size,1)\n",
        "# test = Z[split:].reshape(65,1)\n",
        "\n",
        "n_steps = 5\n",
        "n_output = 1\n",
        "training_size = split_size - (n_steps+n_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dcb16cd",
      "metadata": {
        "id": "3dcb16cd"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((training_size,n_steps,1))\n",
        "y = np.zeros((training_size,n_output,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9920445e",
      "metadata": {
        "id": "9920445e"
      },
      "outputs": [],
      "source": [
        "for i in range(len(x)):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "    # check if we are beyond the sequence\n",
        "    if end_ix > len(train)-1:\n",
        "        break\n",
        "    # gather input and output parts of the pattern\n",
        "    seq_x, seq_y = train[i:end_ix], train[end_ix:(end_ix+n_output)]\n",
        "    x[i] = seq_x\n",
        "    y[i] = seq_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed11c895",
      "metadata": {
        "id": "ed11c895"
      },
      "outputs": [],
      "source": [
        "# indexes = np.arange(x.shape[0])\n",
        "# train_index = indexes[: int(0.95 * x.shape[0])]\n",
        "# val_index = indexes[int(0.95 * x.shape[0]) :]\n",
        "\n",
        "# x_train, y_train = x[train_index],y[train_index]\n",
        "# x_val, y_val = x[val_index],y[val_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df4be50",
      "metadata": {
        "id": "7df4be50"
      },
      "outputs": [],
      "source": [
        "\n",
        "def model_function(q,x,y):\n",
        "    def tilted_loss(y,f):\n",
        "\n",
        "        e1 = (y-f)\n",
        "        the_sum = (Kr.mean(Kr.maximum(q*e1, (q-1)*e1), axis=-1))\n",
        "        return the_sum\n",
        "\n",
        "    # define model\n",
        "#     n_steps = 25\n",
        "    n_features = 1\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
        "    # model.add(Dropout(0.1))\n",
        "    # model.add(LSTM(50, activation='relu',return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(50, activation='relu'))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(Dense(50, input_dim = x_train.shape[1],\n",
        "    #                 kernel_initializer='he_uniform', activation='relu'))\n",
        "    # model.add(Dense(50, activation='relu'))\n",
        "    # # model.add(Dense(50, activation='relu'))\n",
        "    # model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(n_output,activation='linear'))\n",
        "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "    model.compile(loss=tilted_loss, optimizer=optimizer)\n",
        "    print(\"running model for quantile level \"+str(q)+\".\")\n",
        "    result1 = model.fit(x, y, #callbacks = callbacks,\n",
        "                    batch_size = 128,\n",
        "          validation_split = 0.05, epochs=120, verbose=0)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b783c253",
      "metadata": {
        "scrolled": true,
        "id": "b783c253"
      },
      "outputs": [],
      "source": [
        "# callbacks = [EarlyStopping(monitor='val_loss', patience=330),\n",
        "#          ModelCheckpoint(filepath='lstm_2a_3.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "model1 = model_function(0.05,x,y)\n",
        "model2 = model_function(0.5,x,y)\n",
        "model3 = model_function(0.95,x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0067c4df",
      "metadata": {
        "id": "0067c4df"
      },
      "outputs": [],
      "source": [
        "Z_test = Z[(split_size-n_steps):split_size].reshape(1,n_steps,1)\n",
        "pred05 = np.empty((0,1))\n",
        "pred5 = np.empty((0,1))\n",
        "pred95 = np.empty((0,1))\n",
        "\n",
        "for i in range(5):\n",
        "    pred1 = model1.predict(Z_test)\n",
        "    pred2 = model2.predict(Z_test)\n",
        "    pred3 = model3.predict(Z_test)\n",
        "\n",
        "    Z_test = np.append(Z_test[:,:(n_steps-1),:],pred2).reshape(1,n_steps,1)\n",
        "    pred05 = np.append(pred05,pred1)\n",
        "    pred5 = np.append(pred5,pred2)\n",
        "    pred95 = np.append(pred95,pred3)\n",
        "\n",
        "mean_pred = np.empty((0,1))\n",
        "\n",
        "for i in range(len(x)):\n",
        "    if i == (training_size - 1):\n",
        "        pred2 = model2.predict(x[i,:,:].reshape(1,n_steps,1))\n",
        "        mean_pred = np.append(mean_pred,pred2)\n",
        "    else:\n",
        "        pred2 = model2.predict(x[i,:,:].reshape(1,n_steps,1))\n",
        "        mean_pred = np.append(mean_pred,pred2[:,0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c15f74",
      "metadata": {
        "id": "b9c15f74"
      },
      "outputs": [],
      "source": [
        "test = Z[split_size:].reshape(5,1)\n",
        "mse = mean_squared_error(pred5, test)\n",
        "mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2e0119",
      "metadata": {
        "id": "fd2e0119"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(500), Z,label = \"true\")\n",
        "\n",
        "# plt.scatter(range(959), x_train[:,0])\n",
        "# plt.scatter(range(959), x_train[:,1])\n",
        "plt.plot(range(6,495), mean_pred, label = \"model mean\")\n",
        "plt.plot(range(495,500), pred05, label = \"model 0.05\")\n",
        "plt.plot(range(495,500), pred5, label = \"model 0.5\")\n",
        "plt.plot(range(495,500), pred95, label = \"model 0.95\")\n",
        "# plt.plot(range(3,100), pred_list95, label = \"model 0.95\")\n",
        "\n",
        "# plt.plot(range(97,100), pred_list[0,:], label = \"model 0.05\")\n",
        "# plt.plot(range(97,100), pred_list[1,:], label = \"model 0.5\")\n",
        "# plt.plot(range(97,100), pred_list[2,:], label = \"model 0.95\")\n",
        "\n",
        "plt.axvline(x=495)\n",
        "# plt.plot(range(959), pred_list[:,1])\n",
        "# plt.plot(range(959), pred_list[:,2])\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0471ff6e",
      "metadata": {
        "scrolled": true,
        "id": "0471ff6e"
      },
      "outputs": [],
      "source": [
        "###### Run the model for all 50 time points #########\n",
        "mse = np.empty((0,1))\n",
        "mpiw = np.empty((0,1))\n",
        "for ind in range(50):\n",
        "\n",
        "    Z = np.array(df.iloc[:,ind])\n",
        "\n",
        "    split_size = 495\n",
        "    train = Z[:split_size].reshape(split_size,1)\n",
        "    # test = Z[split:].reshape(65,1)\n",
        "\n",
        "    n_steps = 5\n",
        "    n_output = 1\n",
        "    training_size = split_size - (n_steps+n_output)\n",
        "    x = np.zeros((training_size,n_steps,1))\n",
        "    y = np.zeros((training_size,n_output,1))\n",
        "    for i in range(len(x)):\n",
        "    # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(train)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = train[i:end_ix], train[end_ix:(end_ix+n_output)]\n",
        "        x[i] = seq_x\n",
        "        y[i] = seq_y\n",
        "\n",
        "    model1 = model_function(0.05,x,y)\n",
        "    model2 = model_function(0.5,x,y)\n",
        "    model3 = model_function(0.95,x,y)\n",
        "    Z_test = Z[(split_size-n_steps):split_size].reshape(1,n_steps,1)\n",
        "    pred05 = np.empty((0,1))\n",
        "    pred5 = np.empty((0,1))\n",
        "    pred95 = np.empty((0,1))\n",
        "\n",
        "    for i in range(5):\n",
        "        pred1 = model1.predict(Z_test)\n",
        "        pred2 = model2.predict(Z_test)\n",
        "        pred3 = model3.predict(Z_test)\n",
        "\n",
        "        Z_test = np.append(Z_test[:,:(n_steps-1),:],pred2).reshape(1,n_steps,1)\n",
        "        pred05 = np.append(pred05,pred1)\n",
        "        pred5 = np.append(pred5,pred2)\n",
        "        pred95 = np.append(pred95,pred3)\n",
        "\n",
        "#     for i in range(len(x)):\n",
        "#         if i == 429:\n",
        "#             pred2 = model2.predict(x[i,:,:].reshape(1,15,1))\n",
        "#             mean_pred = np.append(mean_pred,pred2)\n",
        "#         else:\n",
        "#             pred2 = model2.predict(x[i,:,:].reshape(1,15,1))\n",
        "#             mean_pred = np.append(mean_pred,pred2[:,0])\n",
        "    mpiw = np.append(mpiw,np.mean(pred95 - pred05))\n",
        "    test = Z[split_size:].reshape(n_steps,1)\n",
        "    mse = np.append(mse,mean_squared_error(pred5, test))\n",
        "    print(ind)\n",
        "    print(mse)\n",
        "    print(mpiw)\n",
        "    print(\"################\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2c6bd2",
      "metadata": {
        "id": "5a2c6bd2"
      },
      "outputs": [],
      "source": [
        "pred05 = np.append(mean_pred,pred05)\n",
        "pred95 = np.append(mean_pred,pred95)\n",
        "\n",
        "pred5 = np.append(mean_pred,pred5)\n",
        "\n",
        "z = Z[6:]\n",
        "df_pred = pd.DataFrame()\n",
        "\n",
        "df_pred[\"z\"] = z\n",
        "df_pred[\"interval_05\"] = pred05\n",
        "df_pred[\"interval_5\"] = pred5\n",
        "df_pred[\"interval_95\"] = pred95\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5802c9ec",
      "metadata": {
        "id": "5802c9ec"
      },
      "outputs": [],
      "source": [
        "df_pred.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c048a231",
      "metadata": {
        "id": "c048a231"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv(\"simulation_time_series_loc2.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}